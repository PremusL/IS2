{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ijson\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#podatki iz jsona\n",
    "\n",
    "file_path = \"./News_Category_Dataset_IS_course.json\"\n",
    "data = [json.loads(line, object_hook=lambda o: str(o) if isinstance(o, (str, None))else o) for line in open(file_path, 'r')]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naredi dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "categories = df['category'].value_counts()\n",
    "\n",
    "num_categories = len(categories)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the whole story to the dataframe\n",
    "\n",
    "def add_story(df):\n",
    "    text_column = []\n",
    "\n",
    "    for i in range(15360, len(df)):\n",
    "        # print(f\"index: {i}\")\n",
    "        short_description = df['short_description'].iloc[i]\n",
    "\n",
    "        if (type(short_description) != str):\n",
    "            # print(short_description)\n",
    "            print(f\"index {i}\")\n",
    "            link = df['link'].iloc[i]\n",
    "            try:\n",
    "                response = requests.get(link)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    # section = soup.find('section')\n",
    "                    cur_text_arr = []\n",
    "                    all_data_article = soup.find_all('div', class_='primary-cli') #all the text data in article that could be useful (the last few aren't)\n",
    "                    for i in range(len(all_data_article) - 2):\n",
    "                        k = all_data_article[i]\n",
    "                        cur_text_arr.append(k.text)\n",
    "                    current_string = \" \".join(cur_text_arr)\n",
    "\n",
    "                    df.at[i, 'short_description'] = current_string\n",
    "            \n",
    "            except:\n",
    "                continue\n",
    "                # text_column.append(current_string)\n",
    "\n",
    "    \n",
    "\n",
    "    # df['story'] = text_column\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# links\n",
    "\n",
    "stories = add_story(df)\n",
    "\n",
    "stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.to_csv(\"fixed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    text = str(text)\n",
    "    words = word_tokenize(text.lower())  # Convert text to lowercase\n",
    "\n",
    "    # Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.translate(table) for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Stemming (uncomment if you want to use stemming)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the words back into a string\n",
    "    preprocessed_text = ' '.join(lemmatized_words)\n",
    "    return preprocessed_text\n",
    "\n",
    "df = pd.read_csv('./fixed_data.csv', sep=',')\n",
    "# df\n",
    "df['cleaned_text'] = df['short_description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"fixed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./fixed_data.csv\", sep=',')\n",
    "data['cleaned_text'] = data['cleaned_text'].fillna('')\n",
    "data['cleaned_text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['cleaned_text', 'short_description', 'headline']], data['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer()  # Use TF-IDF vectorizer for text to numerical feature conversion\n",
    "X_train_vec = vectorizer.fit_transform(X_train['cleaned_text'])\n",
    "X_test_vec = vectorizer.transform(X_test['cleaned_text'])\n",
    "\n",
    "tokenized_train_text = [text.split() for text in X_train['cleaned_text']]\n",
    "tokenized_test_text = [text.split() for text in X_test['cleaned_text']]\n",
    "\n",
    "\n",
    "# tokenized_test_text\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Logistic Regression model\n",
    "# logistic_model = LogisticRegression(max_iter=4000)\n",
    "# logistic_model.fit(X_train_vec, y_train)\n",
    "# logistic_predictions = logistic_model.predict(X_test_vec)\n",
    "# logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "# print(\"Logistic Regression Accuracy:\", logistic_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest model slabsi je\n",
    "# rf_model = RandomForestClassifier()\n",
    "# rf_model.fit(X_train_vec, y_train)\n",
    "# rf_predictions = rf_model.predict(X_test_vec)\n",
    "# rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "# print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(tokenized_train_text, vector_size=100, window=5, min_count=1, workers=6, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = w2v_model.wv.index_to_key\n",
    "\n",
    "\n",
    "categories = [cat.lower() for cat in data['category'].unique().tolist()]\n",
    "print(categories)\n",
    "\n",
    "word_vectors_dict = {word: w2v_model.wv[word] for word in all_words}\n",
    "# category_vectors = [w2v_model.wv[word] for word in categories]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = [w2v_model.wv[word] for word in all_words]\n",
    "print(word_vectors)\n",
    "\n",
    "\n",
    "# logistic_model = LogisticRegression()\n",
    "# logistic_model.fit(word_vectors, y_train)\n",
    "# logistic_predictions = logistic_model.predict(X_test_vec)\n",
    "# logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "# print(\"Logistic Regression Accuracy:\", logistic_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text, model):\n",
    "    words = word_tokenize(text.lower())\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if not vectors:\n",
    "        return None\n",
    "    return sum(vectors) / len(vectors)\n",
    "\n",
    "tokenized_texts = [word_tokenize(text.lower()) for text in data['cleaned_text']]\n",
    "model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, workers=6, epochs=10)\n",
    "\n",
    "# Assuming 'texts' is a list of sentences\n",
    "vectors = [text_to_vector(text, model) for text in data['cleaned_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'labels' is a list of class labels\n",
    "cat = data['category']\n",
    "# print(cat, len(cat), len(vectors))\n",
    "filter_vec = list(filter(lambda v: v is not None, vectors))\n",
    "filter_cat = [cat.iloc[i] for i,v in enumerate(vectors) if v is not None]\n",
    "X_train, X_test, y_train, y_test = train_test_split(filter_vec, np.array(filter_cat), test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier\n",
    "classifier = LogisticRegression(max_iter=4000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
